{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enron Submission Questions\n",
    "A critical part of machine learning is making sense of your analysis process and communicating it to others. The questions below will help us understand your decision-making process and allow us to give feedback on your project. Please answer each question; your answers should be about 1-2 paragraphs per question. If you find yourself writing much more than that, take a step back and see if you can simplify your response!\n",
    "\n",
    "When your evaluator looks at your responses, he or she will use a specific list of rubric items to assess your answers. Here is the link to that rubric: [Link] Each question has one or more specific rubric items associated with it, so before you submit an answer, take a look at that part of the rubric. If your response does not meet expectations for all rubric points, you will be asked to revise and resubmit your project. Make sure that your responses are detailed enough that the evaluator will be able to understand the steps you took and your thought processes as you went through the data analysis. \n",
    "\n",
    "\n",
    "Once you’ve submitted your responses, your coach will take a look and may ask a few more focused follow-up questions on one or more of your answers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, a significant amount of typically confidential information entered into the public record, including tens of thousands of emails and detailed financial data for top executives. As part of this project, I have used machine learning algorithms to build person of interest identifier based on financial and email data available to me  in the form of dictionary, where each key-value pair in the dictionary corresponds to one person. The dictionary key is the person's name, and the value is another dictionary, which contains the names of all the features and their values for that person. The features in the data fall into three major types, namely financial features, email features and POI labels.\n",
    "\n",
    "\n",
    "1) There are total 146 datapoints in the dataset\n",
    "('Total data point in dataset :', 146)\n",
    "\n",
    "2) Features used - Considering the fact that email address is used as a unique identifier for any person like his name, it would not have helped in identifying poi. Hence, removed email_address from feature_list\n",
    "Removed 1 email_feature : email_address\n",
    "Total new features created : 3\n",
    "Total  features used : 22\n",
    "\n",
    "3) Plot the scatter plot between salary and bonus of each datapoint in data. The plot and data showed that the data point having salary around 26 million is the total of all the salaries. Removed the key 'TOTAL' from dictionary.\n",
    "Another highest salary is of SKILLING JEFFREY K who is one of poi\n",
    "\n",
    "Salary\n",
    "('SKILLING JEFFREY K', 1111258)\n",
    "('TOTAL', 26704229)\n",
    "\n",
    "After removing Total\n",
    "('LAY KENNETH L', 1072321)\n",
    "('SKILLING JEFFREY K', 1111258)\n",
    "\n",
    "As part of this project, I picked up appropriate set of features, scaled them. Tried and tuned multiple machine learning algorithms and tested and evaluated the identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used SelectKBest algorithm, that selected following features from the list-\n",
    "\n",
    "Decision_Tree\n",
    "12 best parameters with scores:\n",
    "salary 15.806\n",
    "total_payments 8.963\n",
    "loan_advances 7.038\n",
    "bonus 30.652\n",
    "deferred_income 8.493\n",
    "total_stock_value 10.815\n",
    "exercised_stock_options 9.956\n",
    "long_term_incentive 7.535\n",
    "restricted_stock 8.051\n",
    "from_poi_to_this_person 4.939\n",
    "shared_receipt_with_poi 10.670\n",
    "fraction_to_poi_total 13.791\n",
    "\n",
    "Naive_Bayes\n",
    "6 best parameters with scores:\n",
    "salary 15.806\n",
    "bonus 30.652\n",
    "total_stock_value 10.815\n",
    "exercised_stock_options 9.956\n",
    "shared_receipt_with_poi 10.670\n",
    "fraction_to_poi_total 13.791\n",
    "\n",
    "\n",
    "SVM\n",
    "8 best parameters with scores:\n",
    "salary 15.806\n",
    "total_payments 8.963\n",
    "bonus 30.652\n",
    "deferred_income 8.493\n",
    "total_stock_value 10.815\n",
    "exercised_stock_options 9.956\n",
    "shared_receipt_with_poi 10.670\n",
    "fraction_to_poi_total 13.791\n",
    "\n",
    "Adaboost\n",
    "8 best parameters with scores:\n",
    "salary 15.806\n",
    "total_payments 8.963\n",
    "bonus 30.652\n",
    "deferred_income 8.493\n",
    "total_stock_value 10.815\n",
    "exercised_stock_options 9.956\n",
    "shared_receipt_with_poi 10.670\n",
    "fraction_to_poi_total 13.791\n",
    "\n",
    "Random_Forest\n",
    "7 best parameters with scores:\n",
    "salary 15.806\n",
    "total_payments 8.963\n",
    "bonus 30.652\n",
    "total_stock_value 10.815\n",
    "exercised_stock_options 9.956\n",
    "shared_receipt_with_poi 10.670\n",
    "fraction_to_poi_total 13.791\n",
    "\n",
    "\n",
    "K_Nearest_Neighbors\n",
    "5 best parameters with scores:\n",
    "salary 15.806\n",
    "bonus 30.652\n",
    "total_stock_value 10.815\n",
    "shared_receipt_with_poi 10.670\n",
    "fraction_to_poi_total 13.791\n",
    "\n",
    "\n",
    "Yes, I used MinMaxScaler as the parameters were large sums of money, which would have overwhelmed the scale of the other parameters that dealt with email counts.\n",
    "\n",
    "\n",
    "I made three new features:\n",
    "  - fraction_from_poi_total - It is the fraction of emails from POI to total number of emails recieved. The reason of using this feature is to see if people who receive a high percentage of their emails from POIs are in fact themselves POIs. This feature was not used in the final classification algorithm.\n",
    "\n",
    " -  fraction_to_poi_total ­ It is the fraction of emails sent to  POI to total number of emails sent. I added this to see if a person who sends a high percentage of their emails to POIs is himself or herself a POI. In fact,  with a score of 13.791, this was one of the features that I was selected as best feature.\n",
    " \n",
    " - fraction_exercised_stock_total ­ It is the fraction of total number of stocks exercised to total stock values of that person. This feature did not get selected in the best feature. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried 6 different algorithms - Naive Bayes, Support Vector Machine (SVM), Decision Tree Classifier, K Nearest Neighbour, Adaboost and Random Forest. \n",
    "I tuned each algorithm with different set of parameters and tested each of them against test_classifier function in tester script.\n",
    "On comparing the test result of each algorithm when running test_classifier, I looked at Precision, Recall and F1 score. Since Accuracy was virtually identical for all of the algorithms, I chose Naive Bayes and Adaboost because it had the best combination of Precision and Recall and F1.\n",
    "\n",
    "Decision Tree - \n",
    "Precision: 0.26841\t\n",
    "Recall: 0.20050\t\n",
    "F1: 0.22954\n",
    "\n",
    "Naive Bayes\n",
    "Precision: 0.38543\t\n",
    "Recall: 0.30950\t\n",
    "F1: 0.34332\n",
    "\n",
    "SVM \n",
    "Precision: 0.36026\t\n",
    "Recall: 0.13600\t\n",
    "F1: 0.19746\n",
    "\n",
    "Adaboost \n",
    "Precision: 0.40848\t\n",
    "Recall: 0.21200\t\n",
    "F1: 0.27913\t\n",
    "\n",
    "Random Forest\n",
    "Precision: 0.40000\t\n",
    "Recall: 0.15300\t\n",
    "F1: 0.22134\n",
    "\n",
    "K Nearest Neighbours \n",
    "Precision: 0.02083\t\n",
    "Recall: 0.00050\t\n",
    "F1: 0.00098"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4. What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning an algorithm means finding the optimal combination of configuration parameters that yields the highest “score”. In this case, I am looking for the highest weighted F1 score, which combines precision and recall into one value that an algorithm can maximize. \n",
    "I used GridSearchCV, an automated way of running multiple iterations of the same algorithm using different parameter combinations in search of the one that yields the highest score. As mentioned above, in this case, the high score is based on the “F1_Weighted,” During the GridSearchCV step, I used Stratified Shuffle Split on the training labels in an effort to randomize the selection of testing data due to the small sample size.\n",
    "I ultimately selected Naive Bayes, an algorithm that doesn’t have any configuration parameters, because it gives me the best combination of precision and recall for this data set. However, when I tested other algorithms, like Adaboost, for example, I tried to tune them as best I could to see if the F1 score could exceed that of Naive Bayes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation helps in dividing the data into training and testing dataset in such a way that a model trained on training data could perform equally well on data outside the dataset. It also serves as a check on overfitting.\n",
    "The classic mistake of doing validation wrong is accepting hypothesis suggested by a given dataset, even when they are not true.By testing the algorithm on data outside the dataset on which it is trained, we gain more confidence that the result of the algorithm are repatable.\n",
    "\n",
    "I used train_test_split() function to divide may data into training and testing datasets with 30% in test data and random_state as 42. I used test_classifier function of tester script to analyse the performance of my each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used 6 different algorithms and tuned them to give best evaluation metrics. I ran all my classifiers against tester script and got below values for precision, recall and F1 score.\n",
    "Decision Tree - \n",
    "Precision: 0.26841\t\n",
    "Recall: 0.20050\t\n",
    "F1: 0.22954\n",
    "\n",
    "Naive Bayes\n",
    "Precision: 0.38543\t\n",
    "Recall: 0.30950\t\n",
    "F1: 0.34332\n",
    "\n",
    "SVM \n",
    "Precision: 0.36026\t\n",
    "Recall: 0.13600\t\n",
    "F1: 0.19746\n",
    "\n",
    "Adaboost \n",
    "Precision: 0.40848\t\n",
    "Recall: 0.21200\t\n",
    "F1: 0.27913\t\n",
    "\n",
    "Random Forest\n",
    "Precision: 0.40000\t\n",
    "Recall: 0.15300\t\n",
    "F1: 0.22134\n",
    "\n",
    "K Nearest Neighbours \n",
    "Precision: 0.02083\t\n",
    "Recall: 0.00050\t\n",
    "F1: 0.00098\n",
    "\n",
    "Comparing the scores, I am using Naive Bayes and Adabosst algorithms as I can assure that when Naive Bayes predicts the person as POI, 38% of the time it will be a POI.\n",
    "And, if Adaboost predicts the person as POI, 40% of the time it will be a POI.\n",
    "The 0.30 value of recall in Naive Bayes indicatesthat there are 30% chances POI is predicted given the input values are for poi "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
